<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gentle Lau&#39;s Blogs</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://juntaoliu01.github.io/"/>
  <updated>2019-03-31T08:30:19.221Z</updated>
  <id>http://juntaoliu01.github.io/</id>
  
  <author>
    <name>Gentle Lau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>并查集简介与实践</title>
    <link href="http://juntaoliu01.github.io/2019/03/31/%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
    <id>http://juntaoliu01.github.io/2019/03/31/并查集简介与实践/</id>
    <published>2019-03-31T07:52:05.000Z</published>
    <updated>2019-03-31T08:30:19.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="并查集森林"><a href="#并查集森林" class="headerlink" title="并查集森林"></a>并查集森林</h2><p>在计算机科学中，并查集是一种树型的数据结构，用于处理一些不交集（Disjoint Sets）的合并及查询问题。并查集森林是一种将每一个集合以树表示的数据结构，其中每一个节点保存着到它的父节点的引用。在并查集森林中，每个集合的代表即是集合的根节点。主要包含两种操作：  </p><ul><li>“查找”根据其父节点的引用向根行进直到到底树根  </li><li>“联合”将两棵树合并到一起，这通过将一棵树的根连接到另一棵树的根  </li></ul><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>每个节点构成一个树，其父节点就是其本身  </p><pre><code>init(int n){    vector&lt;int&gt; parent = vector&lt;int&gt;(n);    for(int i = 0;i &lt; n;i++)        parent[i] = i;}  </code></pre><h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><p>找到某个节点所在集合的根结点  </p><pre><code>int find(int p){    while(parent[p] != p)        p = parent[p];    return p;}  </code></pre><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><p>讲两个节点所在集合进行合并  </p><pre><code>void union(int p,int q){    int rootp = find(p);    int rootq = find(q);    if(rootp == rootq)        return;    parent[rootp] = rootq;}</code></pre><p>这是并查集森林的最基础的表示方法，这个方法不会比链表法好，这是因为创建的树可能会严重不平衡；然而，可以用两种办法优化。  </p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><h3 id="按大小合并"><a href="#按大小合并" class="headerlink" title="按大小合并"></a>按大小合并</h3><p>在合并时总是将更小的树连接至更大的树上。因为影响运行时间的是树的深度，更小的树添加到更大的树的根上会扩张树的广度，减少树的深度，提高查找效率。只使用这个方法将使每个Union或Find操作最坏的运行时间提高至 <code>O（log n)</code>。   </p><p>优化后的代码L：  </p><pre><code>## 初始化init (int n){    vector&lt;int&gt; size = vector&lt;int&gt;(n);    vector&lt;int&gt; parent = vector&lt;int&gt;(n);    for(int i = 0;i &lt; n;i++){        size[i] = 1;        parent[i] = i;    }}## 合并  void union_(int p,int q){    int rootp = find(p);    int rootq = find(q);    if(rootp == rootq)        return;    if(size[rootp] &lt; size[rootq]){        parent[rootp] = rootq;        size[rootq] += size[rootp];    }    else{        parent[rootq] = rootp;        size[rootp] += size[rootq];    }}</code></pre><h3 id="路径压缩"><a href="#路径压缩" class="headerlink" title="路径压缩"></a>路径压缩</h3><p>在执行“查找”时扁平化树结构的方法。关键在于在路径上的每个节点都可以直接连接到根上；他们都有同样的表示方法。为了达到这样的效果，Find递归地经过树，改变每一个节点的引用到根节点。得到的树将更加扁平，为以后直接或者间接引用节点的操作加速。  </p><p>优化后的代码：  </p><pre><code>int find(int p){     while(parent[p] != p){        parent[p] = parent[parent[p]];        p = parent[p];    }    return p;}</code></pre><h2 id="典型问题"><a href="#典型问题" class="headerlink" title="典型问题"></a>典型问题</h2><p><a href="https://leetcode.com/problems/friend-circles/" target="_blank" rel="noopener"><strong>Friend Circles</strong></a>  </p><p>Description:  </p><p>There are N students in a class. Some of them are friends, while some are not. Their friendship is transitive in nature. For example, if A is a direct friend of B, and B is a direct friend of C, then A is an indirect friend of C. And we defined a friend circle is a group of students who are direct or indirect friends.</p><p>Given a N*N matrix M representing the friend relationship between students in the class. If M[i][j] = 1, then the i-th and j-th students are direct friends with each other, otherwise not. And you have to output the total number of friend circles among all the students.  </p><p>Solution:  </p><pre><code>class Solution {public:    vector&lt;int&gt; parent;    vector&lt;int&gt; size;    int count;    void init(int n){        count = n;        for(int i = 0;i &lt; n;i++){            parent.push_back(i);            size.push_back(1);        }    }    int find(int p){        while(parent[p] != p){            parent[p] = parent[parent[p]];            p = parent[p];        }        return p;    }    void join(int a,int b){        int pa = find(a);        int pb = find(b);        if(pa == pb)            return;        if(size[pa] &lt; size[pb]){            parent[pa] = pb;            size[pb] += size[pa];        }        else{            parent[pb] = pa;            size[pa] += size[pb];        }        count--;    }    int findCircleNum(vector&lt;vector&lt;int&gt;&gt;&amp; M) {        int n = M.size();        init(n);        for(int i = 0;i &lt; n;i++){            for(int j = i+1;j &lt; n;j++){                if(M[i][j])                    join(i,j);            }        }        return count;    }};</code></pre><p>类似习题：  </p><ul><li><a href="https://leetcode.com/problems/regions-cut-by-slashes/" target="_blank" rel="noopener">Regions Cut By Slashes</a>  </li><li><a href="https://leetcode.com/problems/redundant-connection/" target="_blank" rel="noopener">Redundant Connection</a>  </li><li><a href="https://leetcode.com/problems/largest-component-size-by-common-factor/" target="_blank" rel="noopener">Largest Component Size by Common Factor</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;并查集森林&quot;&gt;&lt;a href=&quot;#并查集森林&quot; class=&quot;headerlink&quot; title=&quot;并查集森林&quot;&gt;&lt;/a&gt;并查集森林&lt;/h2&gt;&lt;p&gt;在计算机科学中，并查集是一种树型的数据结构，用于处理一些不交集（Disjoint Sets）的合并及查询问题。并查集
      
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://juntaoliu01.github.io/tags/Leetcode/"/>
    
      <category term="算法" scheme="http://juntaoliu01.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>使用 gensim 训练中文词向量 (word2vec)</title>
    <link href="http://juntaoliu01.github.io/2019/03/29/%E4%BD%BF%E7%94%A8-gensim-%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F-word2vec/"/>
    <id>http://juntaoliu01.github.io/2019/03/29/使用-gensim-训练中文词向量-word2vec/</id>
    <published>2019-03-29T02:45:37.000Z</published>
    <updated>2019-03-29T03:10:44.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>word2vec算法通过将语词(word)映射到N维的向量空间，然后基于这个词向量可以进行聚类，找到近似词以及词性分析等相关的应用。</p><p>关于word2vec原理和核心算法CBOW（Continuous Bag-Of- Words），Skip-Gram在 <a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/8.2-word2vec-concept-introduction.ipynb" target="_blank" rel="noopener">Word2vec 詞嵌入 (word embeddings) 的基本概念</a>已经进行了解释，不过对如何训练word2vec的模型并没有太多着墨。</p><p>将使用维基百科的中文语料，并使用python的gensim套件来训练word2vec的模型。  </p><h2 id="依赖库"><a href="#依赖库" class="headerlink" title="依赖库"></a>依赖库</h2><ul><li>Python 2.7  </li><li>gensim  </li><li>jieba  </li><li>hanziconv  </li></ul><p>脚本导入上述库示例如下L：  </p><pre><code>import jiebafrom gensim.corpora import WikiCorpusfrom gensim.models import word2vecfrom gensim.models.keyedvectors import KeyedVectorsfrom hanziconv import HanziConv   </code></pre><p>如遇到库缺失的情况可通过 pip 安装：  </p><pre><code>pip install XXX  </code></pre><h2 id="语料获取"><a href="#语料获取" class="headerlink" title="语料获取"></a>语料获取</h2><p>到 <a href="https://dumps.wikimedia.org/zhwiki/" target="_blank" rel="noopener">中文维基百科dump</a> 的目录下找到最新的dump资料档zhwiki-yyyymmdd-pages-articles.xml.bz2,下载到项目目录下。  </p><h3 id="初始化语料"><a href="#初始化语料" class="headerlink" title="初始化语料"></a>初始化语料</h3><p>通过WikiCorpus初始化预料，然后由get_texts()可迭代每一篇wikipedia的文章，它所回传的是一个tokens list，我们以空白符将这些 tokens 串接起来，统一输出到同一份文本文件里。 </p><pre><code>def wiki2txt(filepath):    wiki_corpus = WikiCorpus(filepath,dictionary={})    with open(&quot;wiki_text.txt&quot;,&quot;w&quot;) as wf:        count = 0        for text in wiki_corpus.get_texts():            text = [sentence.encode(&quot;utf-8&quot;) for sentence in text]            wf.write(&quot; &quot;.join(text) + &quot;\n&quot;)            count += 1            if count % 10000 == 0:                print(&quot;%d articles have been processed&quot; % count)  </code></pre><h3 id="中文分词与stop-word移除"><a href="#中文分词与stop-word移除" class="headerlink" title="中文分词与stop-word移除"></a>中文分词与stop-word移除</h3><p>清完XML标签的语料了，再来就是要把语料中每个句子，进一步拆解成语词，这个步骤称为「分词」。中文分词的工具有很多，这里采用的是jieba。在wiki的中文文档中有简体跟繁体混在一起的情形，所以我们在分词前，还需加上一道简繁转换的手续。  </p><pre><code>def segmentation():    stop_words = set()    with open(&quot;stop_words.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            stop_words.add(line.strip(&quot;\n&quot;))    wf = open(&quot;simplified.txt&quot;,&quot;w&quot;)        with open(&quot;wiki_text.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            wf.write(HanziConv.toSimplified(line).encode(&quot;utf-8&quot;))    wf.close()    wf = open(&quot;segmentation.txt&quot;,&quot;w&quot;)    with open(&quot;simplified.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            line = line.strip(&quot;\n&quot;)            pos = jieba.cut(line,cut_all=False)            for word in pos:                word = word.encode(&quot;utf-8&quot;)                if word not in stop_words:                     wf.write(word + &quot; &quot;)    wf.close()</code></pre><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>这是最简单的部分，同时也是最困难的部分，简单的是程序代码，困难的是词向量效能上的微调与后训练。</p><p>相关参数:</p><ul><li>sentences:    这是要训练的句子集</li><li>size:    这表示的是训练出的词向量会有几维</li><li>alpha:    机器学习中的学习率，这东西会逐渐收敛到 min_alpha</li><li>sg:    sg=1表示采用skip-gram,sg=0 表示采用cbow</li><li>window:    滑动窗口大小</li><li>workers:    线程数目，建议别超过 4</li><li>min_count:    若这个词出现的次数小于<code>min_count</code>，那他就不会被视为训练对象  </li></ul><pre><code>def train():    sentence = word2vec.Text8Corpus(&quot;segmentation.txt&quot;)    model = word2vec.Word2Vec(sentence,size=300,window=10,min_count=5,workers=4,sg=1)    model.wv.save_word2vec_format(&quot;wiki300.model.bin&quot;, binary = True)</code></pre><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><pre><code>def test():    word_vectors = KeyedVectors.load_word2vec_format(&quot;wiki300.model.bin&quot;,binary=True)    query_list=[&quot;校长&quot;]    res = word_vectors.most_similar(query_list[0].decode(&quot;utf-8&quot;), topn = 5)    for item in res:        print(item[0] + &quot;,&quot; + str(item[1]))    query_list=[&quot;爸爸&quot;,&quot;妈妈&quot;]    res = word_vectors.similarity(query_list[0].decode(&quot;utf-8&quot;), query_list[1].decode(&quot;utf-8&quot;))    print(res)    query_list=[&quot;爸爸&quot;,&quot;老公&quot;,&quot;妈妈&quot;]    print(&quot;%s之于%s，如%s之于&quot; % (query_list[0], query_list[1], query_list[2]))    res = word_vectors.most_similar(positive = [query_list[0].decode(&quot;utf-8&quot;), query_list[1].decode(&quot;utf-8&quot;)], negative = [query_list[2].decode(&quot;utf-8&quot;)], topn = 5)    for item in res:        print(item[0] + &quot;,&quot; + str(item[1]))  </code></pre><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>利用 tesorboard   </p><pre><code>def visualize(output_path):    model = KeyedVectors.load_word2vec_format(&quot;wiki300.model.bin&quot;,binary=True)    meta_file = &quot;w2x_metadata.tsv&quot;    placeholder = np.zeros((len(model.wv.index2word),300))    with open(os.path.join(output_path,meta_file),&apos;wb&apos;) as file_metadata:        for i, word in enumerate(model.wv.index2word):            placeholder[i] = model[word]            if word == &quot;&quot;:                print(&quot;Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard&quot;)                file_metadata.write(&quot;&lt;Empty Line&gt;\n&quot;)            else:                file_metadata.write(word.encode(&quot;utf-8&quot;) + &quot;\n&quot;)    sess = tf.InteractiveSession()    embedding = tf.Variable(placeholder,trainable=False,name=&quot;w2x_metadata&quot;)    tf.global_variables_initializer().run()    saver = tf.train.Saver()    writer = tf.summary.FileWriter(output_path,sess.graph)    config = projector.ProjectorConfig()    embed = config.embeddings.add()    embed.tensor_name = &quot;w2x_metadata&quot;    embed.metadata_path = meta_file    projector.visualize_embeddings(writer,config)    saver.save(sess,os.path.join(output_path,&quot;w2x_metadata.ckpt&quot;))    print(&quot;Run `tensorboard --logdir={0}` to run visualize result on tensorboard&quot;.format(output_path))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;word2vec算法通过将语词(word)映射到N维的向量空间，然后基于这个词向量可以进行聚类，找到近似词以及词性分析等相关的应用。&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://juntaoliu01.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
