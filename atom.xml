<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gentle Lau&#39;s Blogs</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-29T03:10:44.787Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Gentle Lau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 gensim 训练中文词向量 (word2vec)</title>
    <link href="http://yoursite.com/2019/03/29/%E4%BD%BF%E7%94%A8-gensim-%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F-word2vec/"/>
    <id>http://yoursite.com/2019/03/29/使用-gensim-训练中文词向量-word2vec/</id>
    <published>2019-03-29T02:45:37.000Z</published>
    <updated>2019-03-29T03:10:44.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>word2vec算法通过将语词(word)映射到N维的向量空间，然后基于这个词向量可以进行聚类，找到近似词以及词性分析等相关的应用。</p><p>关于word2vec原理和核心算法CBOW（Continuous Bag-Of- Words），Skip-Gram在 <a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/8.2-word2vec-concept-introduction.ipynb" target="_blank" rel="noopener">Word2vec 詞嵌入 (word embeddings) 的基本概念</a>已经进行了解释，不过对如何训练word2vec的模型并没有太多着墨。</p><p>将使用维基百科的中文语料，并使用python的gensim套件来训练word2vec的模型。  </p><h2 id="依赖库"><a href="#依赖库" class="headerlink" title="依赖库"></a>依赖库</h2><ul><li>Python 2.7  </li><li>gensim  </li><li>jieba  </li><li>hanziconv  </li></ul><p>脚本导入上述库示例如下L：  </p><pre><code>import jiebafrom gensim.corpora import WikiCorpusfrom gensim.models import word2vecfrom gensim.models.keyedvectors import KeyedVectorsfrom hanziconv import HanziConv   </code></pre><p>如遇到库缺失的情况可通过 pip 安装：  </p><pre><code>pip install XXX  </code></pre><h2 id="语料获取"><a href="#语料获取" class="headerlink" title="语料获取"></a>语料获取</h2><p>到 <a href="https://dumps.wikimedia.org/zhwiki/" target="_blank" rel="noopener">中文维基百科dump</a> 的目录下找到最新的dump资料档zhwiki-yyyymmdd-pages-articles.xml.bz2,下载到项目目录下。  </p><h3 id="初始化语料"><a href="#初始化语料" class="headerlink" title="初始化语料"></a>初始化语料</h3><p>通过WikiCorpus初始化预料，然后由get_texts()可迭代每一篇wikipedia的文章，它所回传的是一个tokens list，我们以空白符将这些 tokens 串接起来，统一输出到同一份文本文件里。 </p><pre><code>def wiki2txt(filepath):    wiki_corpus = WikiCorpus(filepath,dictionary={})    with open(&quot;wiki_text.txt&quot;,&quot;w&quot;) as wf:        count = 0        for text in wiki_corpus.get_texts():            text = [sentence.encode(&quot;utf-8&quot;) for sentence in text]            wf.write(&quot; &quot;.join(text) + &quot;\n&quot;)            count += 1            if count % 10000 == 0:                print(&quot;%d articles have been processed&quot; % count)  </code></pre><h3 id="中文分词与stop-word移除"><a href="#中文分词与stop-word移除" class="headerlink" title="中文分词与stop-word移除"></a>中文分词与stop-word移除</h3><p>清完XML标签的语料了，再来就是要把语料中每个句子，进一步拆解成语词，这个步骤称为「分词」。中文分词的工具有很多，这里采用的是jieba。在wiki的中文文档中有简体跟繁体混在一起的情形，所以我们在分词前，还需加上一道简繁转换的手续。  </p><pre><code>def segmentation():    stop_words = set()    with open(&quot;stop_words.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            stop_words.add(line.strip(&quot;\n&quot;))    wf = open(&quot;simplified.txt&quot;,&quot;w&quot;)        with open(&quot;wiki_text.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            wf.write(HanziConv.toSimplified(line).encode(&quot;utf-8&quot;))    wf.close()    wf = open(&quot;segmentation.txt&quot;,&quot;w&quot;)    with open(&quot;simplified.txt&quot;,&quot;r&quot;) as rf:        for line in rf:            line = line.strip(&quot;\n&quot;)            pos = jieba.cut(line,cut_all=False)            for word in pos:                word = word.encode(&quot;utf-8&quot;)                if word not in stop_words:                     wf.write(word + &quot; &quot;)    wf.close()</code></pre><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>这是最简单的部分，同时也是最困难的部分，简单的是程序代码，困难的是词向量效能上的微调与后训练。</p><p>相关参数:</p><ul><li>sentences:    这是要训练的句子集</li><li>size:    这表示的是训练出的词向量会有几维</li><li>alpha:    机器学习中的学习率，这东西会逐渐收敛到 min_alpha</li><li>sg:    sg=1表示采用skip-gram,sg=0 表示采用cbow</li><li>window:    滑动窗口大小</li><li>workers:    线程数目，建议别超过 4</li><li>min_count:    若这个词出现的次数小于<code>min_count</code>，那他就不会被视为训练对象  </li></ul><pre><code>def train():    sentence = word2vec.Text8Corpus(&quot;segmentation.txt&quot;)    model = word2vec.Word2Vec(sentence,size=300,window=10,min_count=5,workers=4,sg=1)    model.wv.save_word2vec_format(&quot;wiki300.model.bin&quot;, binary = True)</code></pre><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><pre><code>def test():    word_vectors = KeyedVectors.load_word2vec_format(&quot;wiki300.model.bin&quot;,binary=True)    query_list=[&quot;校长&quot;]    res = word_vectors.most_similar(query_list[0].decode(&quot;utf-8&quot;), topn = 5)    for item in res:        print(item[0] + &quot;,&quot; + str(item[1]))    query_list=[&quot;爸爸&quot;,&quot;妈妈&quot;]    res = word_vectors.similarity(query_list[0].decode(&quot;utf-8&quot;), query_list[1].decode(&quot;utf-8&quot;))    print(res)    query_list=[&quot;爸爸&quot;,&quot;老公&quot;,&quot;妈妈&quot;]    print(&quot;%s之于%s，如%s之于&quot; % (query_list[0], query_list[1], query_list[2]))    res = word_vectors.most_similar(positive = [query_list[0].decode(&quot;utf-8&quot;), query_list[1].decode(&quot;utf-8&quot;)], negative = [query_list[2].decode(&quot;utf-8&quot;)], topn = 5)    for item in res:        print(item[0] + &quot;,&quot; + str(item[1]))  </code></pre><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>利用 tesorboard   </p><pre><code>def visualize(output_path):    model = KeyedVectors.load_word2vec_format(&quot;wiki300.model.bin&quot;,binary=True)    meta_file = &quot;w2x_metadata.tsv&quot;    placeholder = np.zeros((len(model.wv.index2word),300))    with open(os.path.join(output_path,meta_file),&apos;wb&apos;) as file_metadata:        for i, word in enumerate(model.wv.index2word):            placeholder[i] = model[word]            if word == &quot;&quot;:                print(&quot;Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard&quot;)                file_metadata.write(&quot;&lt;Empty Line&gt;\n&quot;)            else:                file_metadata.write(word.encode(&quot;utf-8&quot;) + &quot;\n&quot;)    sess = tf.InteractiveSession()    embedding = tf.Variable(placeholder,trainable=False,name=&quot;w2x_metadata&quot;)    tf.global_variables_initializer().run()    saver = tf.train.Saver()    writer = tf.summary.FileWriter(output_path,sess.graph)    config = projector.ProjectorConfig()    embed = config.embeddings.add()    embed.tensor_name = &quot;w2x_metadata&quot;    embed.metadata_path = meta_file    projector.visualize_embeddings(writer,config)    saver.save(sess,os.path.join(output_path,&quot;w2x_metadata.ckpt&quot;))    print(&quot;Run `tensorboard --logdir={0}` to run visualize result on tensorboard&quot;.format(output_path))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;word2vec算法通过将语词(word)映射到N维的向量空间，然后基于这个词向量可以进行聚类，找到近似词以及词性分析等相关的应用。&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
</feed>
